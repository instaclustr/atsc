\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{sectsty}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{ATSC - A novel approach to monitoring time series compression\\}

\author{\IEEEauthorblockN{1\textsuperscript{st} Carlos Rolo}
\IEEEauthorblockA{\textit{Instaclustr (of Aff.)} \\
\textit{Netapp (of Aff.)}\\
Lisbon, Portugal \\
carlos.rolo@netapp.com}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Joshua Varghese}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{Open SI(NetApp)}\\
Canberra, Australia \\
u3227463@uni.canberra.edu.au}
}

\maketitle

\begin{abstract}
    In this research, we present ATSC (Advanced Time Series Compressor), a ground-breaking approach to monitoring time-series compression. Time series data, representing observations of a single entity over varying time intervals, is a critical component in diverse domains such as climate monitoring and system performance analysis. Our methodology diverges from conventional compression strategies that focus on sample properties or small sequence segments. ATSC treats each time series as a digital signal and incorporates proven techniques from audio and signal compression, emphasizing a unique perspective on lossless compression. Notably, we leverage Function Approximation (FA), commonly used for lossy compression in time series, to address the compression challenge from a lossless standpoint.
    Our research addresses the high-frequency monitoring of computer systems, where signals often lack periodicity or harmonic components. By treating signals holistically and applying audio compression techniques, we achieve not only compression benefits but also exploit the streaming functionality inherent in audio formats. This methodology extends beyond at-rest compression, encompassing in-transit scenarios, thereby reducing egress costs in cloud environments.
    Motivated by the escalating costs of storage and data transfer, as well as the need for efficient computation in processing vast datasets, ATSC introduces innovative techniques in both the write and read paths. The write path involves signal pre-processing, unique packing methodologies, and novel signal identification, leading to a compressed file format. The read path stands out by employing a unique indexing system, enabling precise streaming to relevant sections of the file without storing timestamp information, a feature particularly suited for time-series data.
    Preliminary testing against a Prometheus instance demonstrates significant space savings in the ATSC data directory compared to traditional storage methods. Future work involves exploring concepts such as mutual information and Kullback–Leibler divergence to further enhance compression efficiency.
    In conclusion, ATSC emerges as a promising solution, offering substantial compression gains, efficient storage, and improved data accessibility for time series monitoring.
\end{abstract}

\begin{IEEEkeywords}
Timeseries Compression, Function Approximation, Audio Compression, Data Storage, Streaming, Indexing, Computer Systems Monitoring
\end{IEEEkeywords}

\section{Introduction}
The realm of computer systems monitoring, distinct from general time-series monitoring, is characterized by a unique set of challenges. Typically marked by very low-frequency sampling, ranging from 0.05Hz to 1~2Hz, this process focuses on capturing signals from various processes, such as database operations and the overall health of the operating system [10]. 
 
An intriguing facet of this domain is the absence of periodicity or harmonic components in most signals. In many cases, these signals are treated as isolated samples or short sequences, employing compression techniques like XOR compression and delta-delta encoding. 
However, a paradigm shift emerges by viewing these signals collectively. While the techniques themselves may not be novel, their application in this domain is ground-breaking. By harnessing audio packing and processing methodologies, we not only capitalize on compression benefits but also tap into the extensive streaming functionality embedded in audio formats. 
 
The significance of data compression extends beyond storage concerns to encompass in-transit scenarios, especially with the rise of cloud computing. Sending monitoring data to external systems incurs egress costs, and stored data must be uncompressed and processed before transmission. Leveraging audio packing techniques allows us to seamlessly integrate with the broader ecosystem of software and hardware, ranging from servers to mobile devices, proficient in transporting and encoding/decoding such information. 

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Fig 1.}
% \label{fig}
% \end{figure}

Our approach doesn't only yield savings in at-rest scenarios but also in transit and during compute processing. By leveraging client-side devices [9], we enable highly efficient processing, incorporating parallelization techniques [7], and exploiting System-on-a-Chip architectures [8] for enhanced performance. This holistic strategy positions our methodology at the forefront of efficient and resource-conscious computer systems monitoring. 

\section{Background}

In the dynamic landscape of computer systems monitoring, the impetus for our research stems from critical challenges that impact both the efficiency and cost-effectiveness of current practices. 

\subsection{High Storage Cost}

The exponential growth of data in computer systems monitoring has led to soaring storage costs. Traditional methods often struggle to cope with the sheer volume of information generated by processes, databases, and overall system health monitoring. Our research delves into innovative time-series compression techniques to alleviate the burden of high storage costs, offering a sustainable solution for data retention. 

\subsection{High Egress Cos}
Sending monitoring data to external systems, especially in cloud environments, incurs significant egress costs. Our research recognises the financial implications of these expenses and aims to mitigate them through advanced compression methodologies. By optimising data before transmission, we not only reduce egress costs but also enhance the overall efficiency of data transfer. 

\subsection{Balancing Data Reduction and Information Preservation}
To manage overwhelming data volumes, conventional practices often resort to techniques like averaging. While effective in reducing data points, this approach comes at a cost – a loss of valuable information. Our research seeks to address this trade-off by proposing advanced time-series compression methods that achieve data reduction without sacrificing critical insights. 

\subsection{Computational Challenges in Traditional Monitoring Techniques}
Traditional approaches, such as point-walking algorithms, demand substantial computational resources to process the vast amounts of monitoring data. Recognising the strain on computing capabilities, our research introduces a more resource-efficient paradigm. By exploring novel compression techniques inspired by audio processing, we aim to streamline the computational demands of data analysis in computer systems monitoring. 

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Fig 2.}
% \label{fig}
% \end{figure}

Our comprehensive approach not only tackles the immediate challenges of high storage and egress costs but also addresses the inherent trade-offs in data reduction techniques. The overarching goal is to redefine the landscape of computer systems monitoring by introducing a cost-effective and information-rich paradigm through advanced time-series compression. 



\section{Methodology}

\subsection{ATSC Write Paths}

\subsubsection{Pre-processing the Signal for Initial Reduction}
Ahead of compression, the signal undergoes meticulous pre-processing, incorporating techniques such as XOR Compression. This initial reduction step aims to eliminate redundant information and prepare the signal for more advanced compression methodologies.

\vspace{10pt}
\subsubsection{Packing the Signal in a Non-compressed Format (e.g., WAV)}
The uncompressed signal is then packed into a non-compressed format, such as WAV. This step includes intriguing elements like simulating a Doppler Shift in the signal, adding a unique touch to the compression process. Additionally, the signal is strategically split into integer and float types, formatted to sizes compatible with audio sampling. 

\vspace{10pt}
\subsubsection{Identifying and Exploiting Local Opportunities to Mimic Audio Signals}
ATSC leverages its distinctive approach by identifying local opportunities within the signal to mimic audio characteristics. This involves capitalizing on patterns and features that resemble audio signals, setting the stage for enhanced compression in the subsequent stages.

\vspace{10pt}
\subsubsection{Generate an Index for Precision Access}
To facilitate precise access to samples without the need for full file seeking, ATSC generates a comprehensive index. This index ensures efficient retrieval of relevant portions of the signal, a critical aspect in the subsequent compression and decompression processes. It is a unique feature of ATSC, involving a meticulous search for optimal ways to benefit from channel correlation, elevating the compression efficiency beyond traditional approaches.

\vspace{10pt}
\subsubsection{Applying Audio-compatible Compression Techniques}
Once a substantial number of samples are amassed, ATSC applies audio-compatible, open-source license, and non-patented compression techniques. This includes innovative approaches like channel splitting, where the data is divided into several channels to harness existing channel correlation techniques, effectively reducing channel sizes.

\vspace{10pt}
\subsubsection{Blocking and Modelling}
The signal samples are divided into blocks, and for each block, ATSC applies modelling techniques tailored to the specific characteristics of the signal. This may involve techniques such as Linear Predictive Coding (LPC), Polynomial Prediction, Fast Fourier Transforms (FFT), or other methodologies that best suit the signal's behaviour.

\vspace{10pt}
\subsubsection{Generate the Final Compressed File}
Building upon the insights gained from steps 1 to 7, ATSC culminates the write path by generating the final compressed file. This file encapsulates the intricacies of the original signal in a highly compressed and efficient format, ready for storage or transmission.





\subsection{ATSC Read Paths}

Within the Read Path of BRRO, a ground-breaking methodology unfolds, emphasizing the unique utilization of indexing for precise data retrieval. Notably, BRRO distinguishes itself by eschewing the storage of timestamp information directly within the file, opting instead for a distinctive indexing approach that enables efficient streaming and targeted decompression of pertinent data segments.

\vspace{10pt}
\subsubsection{Precise Streaming through Specialized Indexing}\label{SCM}
The core innovation in the Read Path lies in the application of a specialized indexing mechanism. Unlike traditional methods, BRRO's indexing facilitates pinpointing the relevant portion of the file without the necessity of timestamp inclusion in the file itself. This precision in indexing empowers BRRO to streamline the streaming process, focusing exclusively on the required data, thereby optimizing resource utilization. 

\vspace{10pt}
\subsubsection{Breaking Ground in Monitoring Data Retrieval}\label{SCM}
While the standard practice of streaming by blocks and subsequent decompression is commonplace in multimedia file handling, BRRO introduces a paradigm shift when applied to the distinct requirements of monitoring data. This unique approach sets BRRO apart, presenting an innovative means of handling time series data that has not been extensively documented or explored in the monitoring space. 

\vspace{10pt}
\subsubsection{Unveiling of the Read Path}\label{SCM}
\begin{itemize}

\item{\textbf{\textit{Identifying the Metric to be Queried:}}} The Read Path initiation begins with the user specifying the metric of interest. This action triggers the subsequent retrieval process.
\vspace{5pt}

\item{\textbf{\textit{Locate Files and Corresponding Index:}}} ATSC efficiently locates pertinent file(s) and their corresponding indices. Leveraging the indexing system ensures accuracy in file identification.
\vspace{5pt}
\item{\textbf{\textit{Precision in Data Retrieval Using the Index:}}} The index plays a crucial role in ATSC's approach. It aids in precisely identifying the segment of the file essential for the query. This ensures focused and efficient data retrieval.
\vspace{5pt}
\item{\textbf{\textit{Streaming Blocks to the Client:}}} Once the relevant data segment is identified, ATSC initiates streaming of these blocks. Direct streaming optimizes data transfer, particularly beneficial in scenarios with limited bandwidth or transmission constraints.
\vspace{5pt}
\item{\textbf{\textit{Decompression and Extraction by the Client:}}} The client, equipped with the streamed data blocks, handles decompression and extraction. This decentralized approach enhances efficiency by distributing the computational load.
\vspace{5pt}
\item{\textbf{\textit{Timestamp Integration Post-Decompression:}}} Following decompression, the samples, now in their extracted form, receive timestamp information from the index. This final step ensures temporal accuracy and relevance of the retrieved data.
\end{itemize}

\subsection{Indexing Samples with VRSI (Variable Rate Sampling Interval)}

\subsubsection{Overview of VRSI Mechanism}

In the methodology section, a critical aspect of the process involves indexing samples for efficient data retrieval. Variable Rate Sampling Interval (VRSI) is employed to manage the sampling intervals effectively. VRSI operates on the premise that samples occur at evenly spaced intervals, such as every 5 seconds, 20 seconds, or 60 seconds.

\subsubsection{Line Segment Representation}

For each sampling interval, a line segment is created with specific fields:

\begin{itemize}
    \item \textbf{Start Timestamp:} The timestamp marking the beginning of the sampling interval.
    \item \textbf{Sample Interval:} The time gap between consecutive samples.
    \item \textbf{Starting Sample:} The index of the first sample within the segment.
    \item \textbf{Number of Samples:} The total number of samples within the segment.
\end{itemize}

\subsubsection{File Structure}

These line segments are then stored in a file, accompanied by the lowest and highest timestamps in the segments represented. This file structure allows for easy identification of whether a requested interval is present in the index. If the requested interval falls entirely outside the timestamps in the file header, no samples are available for that interval.

\subsubsection{Handling Temporal Gaps}

In cases where a metric stops reporting for a period, a new line segment is generated, ensuring accurate representation of the sampled data over time.

\subsubsection{Example VRSI File Content}

\begin{verbatim}
55745 
59435 
15,0,55745,166 
15,166,58505,63
\end{verbatim}

\begin{itemize}
    \item Line 1) Represents the first timestamp.
    \item Line 2) Represents the last timestamp.
    \item Line segments are detailed below:
    \begin{itemize}
        \item The first line segment has one sample every 15 seconds, starting at timestamp 55745, with a total of 166 samples.
        \item The second line segment also has one sample every 15 seconds, starting at timestamp 58505, with 63 samples.
    \end{itemize}
\end{itemize}

\subsubsection{Locating Samples (Read Path)}

In the read path, when locating a sample in a file using the index, timestamps or timestamp ranges are specified (e.g., "All the samples from 3:30 PM to 4:30 PM"). The system checks if the requested timestamps are within the available timestamps in the file header. If found, the sequence of sample numbers is extracted, indicating the samples needed for decompression.

\subsubsection{Creating/Updating the Index (Write Path)}

When a new sample is added, the index is updated. Since time progresses linearly, and samples occur in sequence, the system only needs to:
\begin{itemize}
    \item Update the last segment's sample number or
    \item Create a new segment.
\end{itemize}

Existing segments are incremented if the sample timestamp is the next in sequence. If a segment does not exist or the timestamp is not the next in sequence for the latest segment, a new segment is created. This approach ensures an efficient and organized index for managing variable rate sampling intervals.

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Fig 3.}
% \label{fig}
% \end{figure}


% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Fig 4.}
% \label{fig}
% \end{figure}




\section{Results}

In our quest to assess the efficiency and effectiveness of BRRO, a comprehensive testing methodology was employed. The ATSC server was configured as both a read and write backend for a Prometheus instance, establishing a practical and relevant testing environment. This Prometheus instance, in turn, was connected to an Instaclustr 3-node Cassandra cluster with a Prometheus endpoint enabled. The testing duration, as indicated in the results, spanned a defined period.

\subsection*{ATSC Single: A Best-Case Scenario Analysis}

Before delving into the detailed results, it is essential to address the ATSC single scenario. This represents an optimal output expected on a server running for an extended period, devoid of headers and enriched with substantial data conducive to efficient compression. Although this scenario does not reflect a realistic measure, it serves as a best-case scenario for the current test. The approach involves consolidating all data from every file into a single file, subsequently compressed.

\subsection*{Data Overview}

\textbf{Raw Data Size:} 261,370,032 bytes \\
\textbf{Compression Statistics:}

\begin{itemize}
    \item \textbf{ATSC Compression:}
    \begin{itemize}
        \item Compressed Size: 4,558,363 bytes
        \item Compression Ratio: 57.34 times
    \end{itemize}
    \item \textbf{LZ4 Compression:}
    \begin{itemize}
        \item Compressed Size: 30,634,699 bytes
        \item Compression Ratio: 8.53 times
    \end{itemize}
    \item \textbf{FLAC Compression:}
    \begin{itemize}
        \item Compressed Size: 46,524,174 bytes
        \item Compression Ratio: 5.62 times
    \end{itemize}
\end{itemize}

\textbf{Signal Count:}

\begin{itemize}
    \item Total Signals: 5,860
\end{itemize}

\textbf{Compression Ratios Analysis:}

\begin{itemize}
    \item Median Compression Ratio: 1,653.82
    \item Average Compression Ratio: 1,979.21
\end{itemize}

 % \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Fig 5.}
% \label{fig}
% \end{figure}

\vspace{10pt}
These results offer a detailed insight into the performance of ATSC across various compression techniques. The data overview provides the baseline, while compression statistics and signal count shed light on the efficiency of ATSC's compression strategies. The analysis of compression ratios, both median and average, further quantifies the compression effectiveness across the tested scenarios. 
 

 % \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Fig 5.}
% \label{fig}
% \end{figure}

\vspace{10pt}
The ATSC Compression exhibited an impressive compression ratio of 57.34 times, showcasing the potential of BRRO in achieving substantial data reduction. Additionally, the LZ4 and FLAC Compression techniques demonstrated notable compression ratios of 8.53 and 5.62 times, respectively, highlighting the versatility of ATSC in adapting to different compression scenarios. 
These results substantiate ATSC's capabilities in optimizing storage space, offering valuable insights into its potential impact on real-world applications. 

\section{FUTURE WORK}

In the pursuit of continual improvement and innovation, the following avenues represent promising areas for future exploration within the context of ATSC.

\subsection{Automated Selections: Embracing AI Advancements}

Enhancing User Experience: Investigate the potential benefits of incorporating automated selections, possibly powered by Artificial Intelligence (AI). This exploration aims to streamline user interactions and optimize compression choices intelligently.

\subsection*{Integration with Time Series Databases/Services Seamless Integration}

Extend ATSC's functionality by integrating seamlessly with Time Series databases and services. This integration holds the promise of providing users with a cohesive and efficient solution for managing Time Series data.

\subsection{Enhanced Functionality}

Explore avenues to enhance ATSC's functionality within the realm of Time Series databases, ensuring that users experience a unified and effective solution for their data management needs.

\subsection{Streaming: Leveraging Insights from FLAC Experiments}

Building on Experience: Draw insights from early experiments, particularly those involving Free Lossless Audio Codec (FLAC). Utilize the lessons learned to inform and refine ATSC's performance in streaming scenarios.

\subsection{Optimizing Streaming Capabilities}

Apply the knowledge gained from FLAC experiments to optimize ATSC's streaming capabilities. This involves fine-tuning the compression strategy to excel in scenarios where real-time or efficient streaming of Time Series data is paramount.

These future directions underscore ATSC's commitment to continuous improvement and adaptation to emerging technologies. By exploring automated selections, integrating with Time Series databases, and leveraging lessons from streaming experiences, ATSC aims to stay at the forefront of efficient and intelligent Time Series data compression. This forward-looking section sets the stage for ongoing advancements and ensures that ATSC remains a versatile and cutting-edge solution in the dynamic landscape of data compression.

% Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
% of ``hard'' references (e.g., \verb|(1)|). That will make it possible
% to combine sections, add equations, or change the order of figures or
% citations without having to go through the file line by line.

% Please don't use the \verb|{eqnarray}| equation environment. Use
% \verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
% environment leaves unsightly spaces around relation symbols.

% Please note that the \verb|{subequations}| environment in {\LaTeX}
% will increment the main equation counter even when there are no
% equation numbers displayed. If you forget that, you might write an
% article in which the equation numbers skip from (17) to (20), causing
% the copy editors to wonder if you've discovered a new method of
% counting.

% {\BibTeX} does not work by magic. It doesn't get the bibliographic
% data from thin air but from .bib files. If you use {\BibTeX} to produce a
% bibliography you must send the .bib files. 

% {\LaTeX} can't read your mind. If you assign the same label to a
% subsubsection and a table, you might find that Table I has been cross
% referenced as Table IV-B3. 

% {\LaTeX} does not have precognitive abilities. If you put a
% \verb|\label| command before the command that updates the counter it's
% supposed to be using, the label will pick up the last counter to be
% cross referenced instead. In particular, a \verb|\label| command
% should not go before the caption of a figure or a table.

% Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
% will not stop equation numbers inside \verb|{array}| (there won't be
% any anyway) and it might stop a wanted equation number in the
% surrounding equation.

% \subsection{Some Common Mistakes}\label{SCM}
% \begin{itemize}
% \item The word ``data'' is plural, not singular.
% \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
% \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
% \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
% \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
% \item Do not confuse ``imply'' and ``infer''.
% \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
% \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
% \end{itemize}
% An excellent style manual for science writers is \cite{b7}.

% \subsection{Authors and Affiliations}
% \textbf{The class file is designed for, but not limited to, six authors.} A 
% minimum of one author is required for all conference articles. Author names 
% should be listed starting from left to right and then moving down to the 
% next line. This is the author sequence that will be used in future citations 
% and by indexing services. Names should not be listed in columns nor group by 
% affiliation. Please keep your affiliations as succinct as possible (for 
% example, do not differentiate among departments of the same organization).

% \subsection{Identify the Headings}
% Headings, or heads, are organizational devices that guide the reader through 
% your paper. There are two types: component heads and text heads.

% Component heads identify the different components of your paper and are not 
% topically subordinate to each other. Examples include Acknowledgments and 
% References and, for these, the correct style to use is ``Heading 5''. Use 
% ``figure caption'' for your Figure captions, and ``table head'' for your 
% table title. Run-in heads, such as ``Abstract'', will require you to apply a 
% style (in this case, italic) in addition to the style provided by the drop 
% down menu to differentiate the head from the text.

% Text heads organize the topics on a relational, hierarchical basis. For 
% example, the paper title is the primary text head because all subsequent 
% material relates and elaborates on this one topic. If there are two or more 
% sub-topics, the next level head (uppercase Roman numerals) should be used 
% and, conversely, if there are not at least two sub-topics, then no subheads 
% should be introduced.

% \subsection{Figures and Tables}
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
% rather than symbols or abbreviations when writing Figure axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 
% ``Temperature/K''.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.

% \section*{References}

% Please number citations consecutively within brackets \cite{b1}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.

% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \end{thebibliography}
% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}